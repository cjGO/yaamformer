{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itobxAwPOyvB",
    "outputId": "389c5696-ef36-4490-f10e-08baae40b331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpjl-5TaWkmv",
    "outputId": "9c99b096-571a-4049-c872-5bf4a00131c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yaamformer'...\n",
      "remote: Enumerating objects: 70, done.\u001b[K\n",
      "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
      "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
      "^Cpacking objects:  31% (22/70), 7.95 MiB | 1.32 MiB/s\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cjGO/yaamformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/glect/pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W3BEBQhMQzy2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6155e5cb697e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myaamformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pytorch/yaamformer/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNERprotein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from yaamformer.utils import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fe8J5ewZRahO"
   },
   "outputs": [],
   "source": [
    "def get_dataset(file_location):# Using readline()\n",
    "    file1 = open(file_location, 'r')\n",
    "    count = 0\n",
    "\n",
    "    Seqs = []\n",
    "    Labels = []\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        #print(count)\n",
    "        # Get next line from file\n",
    "        line = file1.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        labels = literal_eval(line.split('\\t')[2])\n",
    "        seqs = line.split('\\t')[5]\n",
    "        seqs = seqs.replace('\\n','')\n",
    "        assert len(seqs) == len(labels), 'MISMATCH'\n",
    "        Seqs.append(seqs)\n",
    "        Labels.append(labels)\n",
    "        # if line is empty\n",
    "        # end of file is reached\n",
    "    #     if count >20:\n",
    "    #         break\n",
    "    return Seqs,Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wpInOeKbPRhD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "Seqs,Labels = get_dataset('./yaamformer/data/YAAM.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhgbPSuiSZn4",
    "outputId": "38756bcf-9432-41af-f6f7-637ce017050e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residues in total dataset with PTMs 0.02109174836588914\n"
     ]
    }
   ],
   "source": [
    "sum_seq = 0\n",
    "for i in Seqs:\n",
    "  sum_seq += len(i)\n",
    "\n",
    "  sum_rez = 0\n",
    "for i in Labels:\n",
    "  sum_rez += sum(i)\n",
    "\n",
    "print(f'Residues in total dataset with PTMs {sum_rez/sum_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xURiLfxdSiOD"
   },
   "outputs": [],
   "source": [
    "yaam = pd.DataFrame({'seq':Seqs,\n",
    "                     'label':Labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MavTrigFMAJo"
   },
   "outputs": [],
   "source": [
    "yaam['len'] = yaam['seq'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wgnmFzsdMGve"
   },
   "outputs": [],
   "source": [
    "max_length=1024\n",
    "\n",
    "yaam = yaam[yaam['len'] < max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_OlwLJ7MNOE",
    "outputId": "11f49bcf-19e7-4971-a928-417a8db09008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4385\n"
     ]
    }
   ],
   "source": [
    "print(len(yaam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-yMxa_YZoGd",
    "outputId": "f9a05c90-b505-4566-d0f7-bee2c42c14bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label = yaam['label'].values[0]\n",
    "new_label = np.zeros(1024)\n",
    "new_label[:len(test_label)]=test_label\n",
    "new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "89dTABSHZPWJ"
   },
   "outputs": [],
   "source": [
    "def pad_labels(label,max_length=1024):\n",
    "  new_label = np.zeros(max_length)\n",
    "  new_label[:len(label)] = label\n",
    "  return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EajMFh6AZeWu"
   },
   "outputs": [],
   "source": [
    "#pad the labels for batch training\n",
    "yaam['label'] = yaam['label'].apply(lambda x: pad_labels(x, max_length=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "y756Or_sRPVf"
   },
   "outputs": [],
   "source": [
    "#tokenize the sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CkNwt7xwPFRY"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(yaam, test_size=0.2)\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seq</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1690</td>\n",
       "      <td>MDINELIIGAQSADKHTREVAETQLLQWCDSDASQVFKALANVALQ...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1197</td>\n",
       "      <td>MSDLLPLATYSLNVEPYTPVPAIDVTMPITVRITMAALNPEAIDEE...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4338</td>\n",
       "      <td>MSFFNFKAFGRNSKKNKNQPLNVAQPPAMNTIYSSPHSSNSRLSLR...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2789</td>\n",
       "      <td>MSDPVELLKRAEKKGVPSSGFMKLFSGSDSYKFEEAADLCVQAATI...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2491</td>\n",
       "      <td>MSTATIQDEDIKFQRENWEMIRSHVSPIISNLTMDNLQESHRDLFQ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                seq  \\\n",
       "0   1690  MDINELIIGAQSADKHTREVAETQLLQWCDSDASQVFKALANVALQ...   \n",
       "1   1197  MSDLLPLATYSLNVEPYTPVPAIDVTMPITVRITMAALNPEAIDEE...   \n",
       "2   4338  MSFFNFKAFGRNSKKNKNQPLNVAQPPAMNTIYSSPHSSNSRLSLR...   \n",
       "3   2789  MSDPVELLKRAEKKGVPSSGFMKLFSGSDSYKFEEAADLCVQAATI...   \n",
       "4   2491  MSTATIQDEDIKFQRENWEMIRSHVSPIISNLTMDNLQESHRDLFQ...   \n",
       "\n",
       "                                               label   len  \n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1004  \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   411  \n",
       "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   287  \n",
       "3  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   292  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   577  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = test.iloc[0]['seq']\n",
    "test_label = test.iloc[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 908]\n"
     ]
    }
   ],
   "source": [
    "for i in np.where(test_label == 1.0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_dataset(dataframe,fragment_length):\n",
    "    \"\"\"\n",
    "    for curriculum learning ; splits dataset around PTMs\n",
    "    \"\"\"\n",
    "    for S in range(len(dataframe)):\n",
    "\n",
    "        test_seq = dataframe.iloc[S]['seq']\n",
    "        test_label = dataframe.iloc[S]['label']\n",
    "\n",
    "        #find each PTM in label\n",
    "        #for each label check if there is 10 nt to left\n",
    "            #if not find how many\n",
    "\n",
    "        for i in np.where(test_label == 1.0): #for each ptm location\n",
    "            #is_negative\n",
    "            for ptm in i:\n",
    "                if (ptm - segment_size >= 0) & (ptm + segment_size < len(test_seq)) : #simple case\n",
    "\n",
    "                    fragment_seq = test_seq[ptm-segment_size+1:ptm+segment_size]\n",
    "                    fragment_label = test_label[ptm-segment_size+1:ptm+segment_size]\n",
    "                   # print('simple', S, len(fragment_seq), fragment_seq, len(fragment_label), fragment_label)\n",
    "\n",
    "                elif ptm - segment_size < 0:\n",
    "                    left_gap = segment_size + (ptm - segment_size)\n",
    "                    right_gap = segment_size*2 - left_gap\n",
    "                    fragment_seq = test_seq[ptm-left_gap+1:ptm+right_gap]\n",
    "                    fragment_label = test_label[ptm-left_gap+1:ptm+right_gap]\n",
    "\n",
    "\n",
    "                elif ptm + segment_size > len(test_seq):\n",
    "                    right_gap = len(test_seq)-ptm\n",
    "                    left_gap =  segment_size*2 - right_gap\n",
    "                    fragment_seq = test_seq[ptm-left_gap+1:ptm+right_gap]\n",
    "                    fragment_label = test_label[ptm-left_gap+1:ptm+right_gap]\n",
    "\n",
    "                assert len(fragment_seq) == segment_size*2 -1, 'wrong length'\n",
    "                assert len(fragment_label) == segment_size*2 -1, 'wrong length'\n",
    "\n",
    "                if fragment_seq not in fragment_seqs:\n",
    "                    fragment_seqs.append(fragment_seq)\n",
    "                    fragment_labels.append(fragment_label)\n",
    "                    \n",
    "    return pd.DataFrame({'seq':fragment_seq,'label':fragment_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_proteins(dataframe, segment_size):\n",
    "    \"\"\"\n",
    "    pass in a test/train dataframe to segment it into smaller fragments for batch training\n",
    "    \"\"\"\n",
    "    \n",
    "    fragment_seqs = []\n",
    "    fragment_labels = []\n",
    "\n",
    "    for S in range(len(dataframe)):\n",
    "\n",
    "        test_seq = dataframe.iloc[S]['seq']\n",
    "        test_label = dataframe.iloc[S]['label']\n",
    "\n",
    "        #find each PTM in label\n",
    "        #for each label check if there is 10 nt to left\n",
    "            #if not find how many\n",
    "\n",
    "        for i in np.where(test_label == 1.0): #for each ptm location\n",
    "\n",
    "            for ptm in i:\n",
    "                if (ptm - segment_size >= 0) & (ptm + segment_size < len(test_seq)) : #simple case\n",
    "                    fragment_seq = test_seq[ptm-segment_size+1:ptm+segment_size]\n",
    "                    fragment_label = test_label[ptm-segment_size+1:ptm+segment_size]\n",
    "\n",
    "                elif ptm - segment_size < 0: # front end\n",
    "                    left_gap = segment_size + (ptm - segment_size)\n",
    "                    right_gap = segment_size*2 - left_gap\n",
    "                    fragment_seq = test_seq[ptm-left_gap:ptm+right_gap-1]\n",
    "                    fragment_label = test_label[ptm-left_gap:ptm+right_gap-1]\n",
    "\n",
    "\n",
    "                elif ptm + segment_size > len(test_seq): #back end\n",
    "                    right_gap = len(test_seq)-ptm\n",
    "                    left_gap =  segment_size*2 - right_gap\n",
    "                    fragment_seq = test_seq[ptm-left_gap+1:ptm+right_gap]\n",
    "                    fragment_label = test_label[ptm-left_gap+1:ptm+right_gap]\n",
    "\n",
    "\n",
    "\n",
    "                assert len(fragment_seq) == segment_size*2 -1, f'wrong length \\n{S} \\n{fragment_seq} \\n {fragment_label}'\n",
    "                assert len(fragment_label) == segment_size*2 -1, f'wrong length \\n{S} \\n{fragment_seq} \\n {fragment_label}'\n",
    "                assert sum(fragment_label) > 0, f'no label \\n{S} \\n{fragment_seq} \\n {fragment_label}'\n",
    "\n",
    "                if fragment_seq not in fragment_seqs:\n",
    "                    fragment_seqs.append(fragment_seq)\n",
    "                    fragment_labels.append(fragment_label)\n",
    "\n",
    "    return pd.DataFrame({'seq':fragment_seqs,'label':fragment_labels})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HqH_KgLpR54K"
   },
   "outputs": [],
   "source": [
    "train_encodings = [tokenize_protein(x) for x in train['seq']]\n",
    "test_encodings = [tokenize_protein(x) for x in test['seq']]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False, max_length=1024)\n",
    "train_encodings = tokenizer(train_encodings, padding='max_length', max_length=1024, truncation=True)\n",
    "test_encodings = tokenizer(test_encodings, padding='max_length', max_length=1024, truncation=True)#padding=True\n",
    "\n",
    "train_labels = train['label']\n",
    "test_labels = test['label']\n",
    "\n",
    "train_dataset = NERprotein(train_encodings, train_labels)\n",
    "test_dataset = NERprotein(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TioTB1YCSZXl",
    "outputId": "92ca7908-24eb-4f60-be04-8b5ab104fddc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertForTokenClassification\n",
    "import re\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "C8Uigi0OS6eg"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvcDPv7STeQc",
    "outputId": "f3a7155e-d3e0-453e-a65e-1e8b630ef7f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer,padding=True,)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./',          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=4, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=4,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=4,  # evaluation batch size\n",
    "    logging_steps=50,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=50,\n",
    "    gradient_checkpointing = True,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vSEfa3uLZIyy"
   },
   "outputs": [],
   "source": [
    "#trainer.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QAin3lfZYEcp",
    "outputId": "0a27d368-ae6a-4314-d262-068102774f44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3508\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2190\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 258/2190 1:41:41 < 12:47:30, 0.04 it/s, Epoch 1.17/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.094599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.079664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.069943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.064946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.064817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 877\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50/config.json\n",
      "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 877\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./checkpoint-100\n",
      "Configuration saved in ./checkpoint-100/config.json\n",
      "Model weights saved in ./checkpoint-100/pytorch_model.bin\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 877\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./checkpoint-150\n",
      "Configuration saved in ./checkpoint-150/config.json\n",
      "Model weights saved in ./checkpoint-150/pytorch_model.bin\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 877\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./checkpoint-200\n",
      "Configuration saved in ./checkpoint-200/config.json\n",
      "Model weights saved in ./checkpoint-200/pytorch_model.bin\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 877\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./checkpoint-250\n",
      "Configuration saved in ./checkpoint-250/config.json\n",
      "Model weights saved in ./checkpoint-250/pytorch_model.bin\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "finetune_protbert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
