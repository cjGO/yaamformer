{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "itobxAwPOyvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024b4368-afc5-44d2-9662-d103636c4504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 90.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 8.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.2-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=b5bfd993cb5798060eb74d185d677cdace0b804654d5fb19f6a9544b9e48870b\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=73dc8fc80dbd4f28361c1801c07ba90afede1e22a28a42c8d5a8022920528d8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.26 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.2 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.9 yaspin-2.1.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n",
            "Collecting ray\n",
            "  Downloading ray-1.9.2-cp37-cp37m-manylinux2014_x86_64.whl (57.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 57.6 MB 177 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.4.2)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (6.0)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.1.0-py3-none-any.whl (171 kB)\n",
            "\u001b[K     |████████████████████████████████| 171 kB 94.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (4.10.0)\n",
            "Collecting deprecated>=1.2.3\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->redis>=3.5.0->ray) (3.0.6)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.0)\n",
            "Installing collected packages: deprecated, redis, ray\n",
            "Successfully installed deprecated-1.2.13 ray-1.9.2 redis-4.1.0\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=99fffa16c4255c3ac6b6ed6118b3ba83c995d0b13fa432f248caae117a68c67c\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install wandb\n",
        "!pip install tensorboardX\n",
        "!pip install ray\n",
        "!pip install seqeval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpjl-5TaWkmv",
        "outputId": "54d1e888-03cc-4431-b248-f5f58ea63462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yaamformer'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 70 (delta 24), reused 52 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (70/70), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cjGO/yaamformer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k3XVEYUep1X",
        "outputId": "47ee4d4d-dafb-444e-bb55-9f7de4aa6d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglector\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W3BEBQhMQzy2"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
        "from yaamformer.utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PARAMETERS\n",
        "\n",
        "args = {'MAX_LENGTH':1024, #max length sequence to keep in training dataset.\n",
        "                           #pads shorter sequence to this length\n",
        "        'DATA_SEED':42, #random state for splitting datasets w/ sklearn\n",
        "        \n",
        "        }"
      ],
      "metadata": {
        "id": "IupqVkuysLCY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wpInOeKbPRhD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "\n",
        "Seqs,Labels = get_dataset('./yaamformer/data/YAAM.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhgbPSuiSZn4",
        "outputId": "7b65858a-73bf-47c7-cb40-452f8ce3131f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximately 2.1% of amino acids are modified.\n"
          ]
        }
      ],
      "source": [
        "sum_seq = 0\n",
        "for i in Seqs:\n",
        "  sum_seq += len(i)\n",
        "\n",
        "  sum_rez = 0\n",
        "for i in Labels:\n",
        "  sum_rez += sum(i)\n",
        "\n",
        "print(f'Approximately {round(sum_rez/sum_seq,3)*100}% of amino acids are modified.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xURiLfxdSiOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5381cc1a-ec0d-456b-e48c-195f96a26c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 4385 sequences available for model training\n"
          ]
        }
      ],
      "source": [
        "yaam = pd.DataFrame({'seq':Seqs,\n",
        "                     'label':Labels})\n",
        "\n",
        "yaam['len'] = yaam['seq'].map(len) #collect sequence lengths to filter\n",
        "yaam = yaam[yaam['len'] < args['MAX_LENGTH']] #filter based on argument max_length\n",
        "print(f'There are {len(yaam)} sequences available for model training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EajMFh6AZeWu"
      },
      "outputs": [],
      "source": [
        "#pad the labels to match the max_length (all sequences are max_length for training)\n",
        "yaam['label'] = yaam['label'].apply(pad_labels,max_length=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CkNwt7xwPFRY"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(yaam, test_size=0.2, random_state=args['DATA_SEED'])\n",
        "test = test.sample(frac=1)\n",
        "train = train.reset_index()\n",
        "test = test.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLoEq6oYQyJP"
      },
      "outputs": [],
      "source": [
        "#model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HqH_KgLpR54K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1d77dde8-d1b1-4b8a-bb20-f5f376c0f38f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-27e04b8210c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_protein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_protein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rostlab/prot_bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAX_LENGTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAX_LENGTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-27e04b8210c1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_protein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_protein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rostlab/prot_bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAX_LENGTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAX_LENGTH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize_protein' is not defined"
          ]
        }
      ],
      "source": [
        "train_encodings = [tokenize_protein(x) for x in train['seq']]\n",
        "test_encodings = [tokenize_protein(x) for x in test['seq']]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False, max_length=args['MAX_LENGTH'])\n",
        "train_encodings = tokenizer(train_encodings, padding='max_length', max_length=args['MAX_LENGTH'], truncation=True)\n",
        "test_encodings = tokenizer(test_encodings, padding='max_length', max_length=args['MAX_LENGTH'], truncation=True)#padding=True\n",
        "\n",
        "train_labels = train['label']\n",
        "test_labels = test['label']\n",
        "\n",
        "train_dataset = NERprotein(train_encodings, train_labels)\n",
        "test_dataset = NERprotein(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQRDlbGgWVkh",
        "outputId": "3a8aef25-56e7-41a9-ccf8-d4c6733c2622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids\n",
            "token_type_ids\n",
            "attention_mask\n"
          ]
        }
      ],
      "source": [
        "for i in train_encodings:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TioTB1YCSZXl"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer, BertForTokenClassification\n",
        "import re\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Any \n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxSAMxuY0BIW"
      },
      "source": [
        "## HYPER PARAMETER TUNING\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34V48xZa8vIp"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class CustomTrainingArguments(TrainingArguments):\n",
        "\n",
        "    weight_picks_0: float = field(\n",
        "        default=None, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "\n",
        "    weight_picks_1: float = field(\n",
        "        default=None, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomTrainer, self).__init__(*args, **kwargs)\n",
        "        \n",
        "\n",
        "    def _hp_search_setup(self, trial: Any):\n",
        "        try:\n",
        "            trial.pop(\"wandb\", None)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        super(CustomTrainer, self)._hp_search_setup(trial)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        #print('hello!')\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "#        weights = torch.tensor([1.,49.])\n",
        "        weight_picks = torch.tensor([training_args.weight_picks_0,training_args.weight_picks_1])\n",
        "        #print(weight_picks)\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight_picks.to('cuda'))\n",
        "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        # loss = loss_fct(\n",
        "        #     logits.view(-1, self.model.config.num_labels), labels.float().view(-1, self.model.config.num_labels)\n",
        "        # )\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n",
        "def model_init():\n",
        "    model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)\n",
        "    return model\n",
        "\n",
        "def f1(predictions):\n",
        "  class_probs = predictions[0]\n",
        "  true_labels = predictions[1]\n",
        "\n",
        "  f1s = []\n",
        "  for i in range(len(class_probs)):\n",
        "    cp = class_probs[i]\n",
        "    tl = true_labels[i]\n",
        "    pred_labels = [np.argmax(x) for x in class_probs[0]]\n",
        "    f1s.append(f1_score(tl,pred_labels))\n",
        "  return {'f1':np.mean(f1s)}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOxrkj2PwmCc",
        "outputId": "76d60ce3-8bcb-441b-b6f9-cf213978b0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglector\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuXca8nk6_nQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dde54b6-bc1d-496a-e7e9-3b537103221c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/Rostlab/prot_bert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/26014e822b9642c94410e0829f3080e5aa601146ffdff9eaafda9ca18a3394e7.baf557855a8618d0ddfb6c23bfd135bfc38ccf8c3fb099b8df45eb110ccf05e9\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Rostlab/prot_bert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e0fb1fe71ed3ee73280750839b42681fbc1c276201aedc10d743b356f078520a.f49f0d5b042c555d2d68e50f1e01e6466f521dd1302884e79d128904c6698245\n",
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file https://huggingface.co/Rostlab/prot_bert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/26014e822b9642c94410e0829f3080e5aa601146ffdff9eaafda9ca18a3394e7.baf557855a8618d0ddfb6c23bfd135bfc38ccf8c3fb099b8df45eb110ccf05e9\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Rostlab/prot_bert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e0fb1fe71ed3ee73280750839b42681fbc1c276201aedc10d743b356f078520a.f49f0d5b042c555d2d68e50f1e01e6466f521dd1302884e79d128904c6698245\n",
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 3508\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3508\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglector\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/glector/huggingface/runs/lbvwpaiy\" target=\"_blank\">pick1_2.0_smooth0.0_bs1</a></strong> to <a href=\"https://wandb.ai/glector/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3508' max='3508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3508/3508 4:25:58, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.140300</td>\n",
              "      <td>0.096186</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.103500</td>\n",
              "      <td>0.096006</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.108500</td>\n",
              "      <td>0.090987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.080400</td>\n",
              "      <td>0.085511</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.096700</td>\n",
              "      <td>0.086808</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.075400</td>\n",
              "      <td>0.081252</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.079700</td>\n",
              "      <td>0.073358</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.074200</td>\n",
              "      <td>0.069252</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.077500</td>\n",
              "      <td>0.067968</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.075400</td>\n",
              "      <td>0.072067</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.068571</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.069200</td>\n",
              "      <td>0.061814</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.066300</td>\n",
              "      <td>0.063481</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.054900</td>\n",
              "      <td>0.061541</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.061200</td>\n",
              "      <td>0.060228</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.067100</td>\n",
              "      <td>0.058883</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.058100</td>\n",
              "      <td>0.058659</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.055500</td>\n",
              "      <td>0.057820</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.056100</td>\n",
              "      <td>0.056883</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.057545</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.059600</td>\n",
              "      <td>0.055021</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.048000</td>\n",
              "      <td>0.057615</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.054600</td>\n",
              "      <td>0.053828</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.044600</td>\n",
              "      <td>0.056330</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.050600</td>\n",
              "      <td>0.059439</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.046300</td>\n",
              "      <td>0.055282</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.053700</td>\n",
              "      <td>0.054286</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.048800</td>\n",
              "      <td>0.051966</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.051790</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.050015</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.047200</td>\n",
              "      <td>0.051072</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.049200</td>\n",
              "      <td>0.050277</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.040300</td>\n",
              "      <td>0.049799</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.057100</td>\n",
              "      <td>0.050002</td>\n",
              "      <td>0.012883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.039700</td>\n",
              "      <td>0.049366</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.052200</td>\n",
              "      <td>0.048272</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.053500</td>\n",
              "      <td>0.053211</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.051300</td>\n",
              "      <td>0.050062</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.046900</td>\n",
              "      <td>0.049523</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.052700</td>\n",
              "      <td>0.051090</td>\n",
              "      <td>0.004492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.056600</td>\n",
              "      <td>0.051096</td>\n",
              "      <td>0.012694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.045400</td>\n",
              "      <td>0.047517</td>\n",
              "      <td>0.006056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.044500</td>\n",
              "      <td>0.051042</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.041300</td>\n",
              "      <td>0.047154</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.044100</td>\n",
              "      <td>0.047560</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.042700</td>\n",
              "      <td>0.047281</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.052400</td>\n",
              "      <td>0.047154</td>\n",
              "      <td>0.010299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.045600</td>\n",
              "      <td>0.046885</td>\n",
              "      <td>0.010299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.048200</td>\n",
              "      <td>0.047939</td>\n",
              "      <td>0.010299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.047674</td>\n",
              "      <td>0.009829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.049200</td>\n",
              "      <td>0.048354</td>\n",
              "      <td>0.009829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.046000</td>\n",
              "      <td>0.046946</td>\n",
              "      <td>0.008093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.045200</td>\n",
              "      <td>0.046433</td>\n",
              "      <td>0.012245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.046790</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.050200</td>\n",
              "      <td>0.046495</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>0.045011</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.045300</td>\n",
              "      <td>0.045652</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.036700</td>\n",
              "      <td>0.045093</td>\n",
              "      <td>0.012913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.052400</td>\n",
              "      <td>0.045060</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.040500</td>\n",
              "      <td>0.044452</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.046200</td>\n",
              "      <td>0.044005</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.039500</td>\n",
              "      <td>0.043966</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.037900</td>\n",
              "      <td>0.044086</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.039200</td>\n",
              "      <td>0.044220</td>\n",
              "      <td>0.007157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.040900</td>\n",
              "      <td>0.043969</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.055900</td>\n",
              "      <td>0.043717</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.045000</td>\n",
              "      <td>0.043540</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.043584</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.040900</td>\n",
              "      <td>0.043541</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.043200</td>\n",
              "      <td>0.043751</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-200\n",
            "Configuration saved in ./checkpoint-200/config.json\n",
            "Model weights saved in ./checkpoint-200/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-200/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-400\n",
            "Configuration saved in ./checkpoint-400/config.json\n",
            "Model weights saved in ./checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-400/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-600\n",
            "Configuration saved in ./checkpoint-600/config.json\n",
            "Model weights saved in ./checkpoint-600/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-600/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-600/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-800\n",
            "Configuration saved in ./checkpoint-800/config.json\n",
            "Model weights saved in ./checkpoint-800/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-800/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-800/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-1000\n",
            "Configuration saved in ./checkpoint-1000/config.json\n",
            "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-1200\n",
            "Configuration saved in ./checkpoint-1200/config.json\n",
            "Model weights saved in ./checkpoint-1200/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-1200/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-1200/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-1400\n",
            "Configuration saved in ./checkpoint-1400/config.json\n",
            "Model weights saved in ./checkpoint-1400/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-1400/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-1400/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-1600\n",
            "Configuration saved in ./checkpoint-1600/config.json\n",
            "Model weights saved in ./checkpoint-1600/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-1600/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-1600/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-1800\n",
            "Configuration saved in ./checkpoint-1800/config.json\n",
            "Model weights saved in ./checkpoint-1800/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-1800/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-1800/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-2000\n",
            "Configuration saved in ./checkpoint-2000/config.json\n",
            "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-2200\n",
            "Configuration saved in ./checkpoint-2200/config.json\n",
            "Model weights saved in ./checkpoint-2200/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-2200/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-2200/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-2400\n",
            "Configuration saved in ./checkpoint-2400/config.json\n",
            "Model weights saved in ./checkpoint-2400/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-2400/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-2400/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-2600\n",
            "Configuration saved in ./checkpoint-2600/config.json\n",
            "Model weights saved in ./checkpoint-2600/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-2600/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-2600/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-2800\n",
            "Configuration saved in ./checkpoint-2800/config.json\n",
            "Model weights saved in ./checkpoint-2800/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-2800/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-2800/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-3000\n",
            "Configuration saved in ./checkpoint-3000/config.json\n",
            "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-3200\n",
            "Configuration saved in ./checkpoint-3200/config.json\n",
            "Model weights saved in ./checkpoint-3200/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-3200/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-3200/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "Saving model checkpoint to ./checkpoint-3400\n",
            "Configuration saved in ./checkpoint-3400/config.json\n",
            "Model weights saved in ./checkpoint-3400/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-3400/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-3400/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 877\n",
            "  Batch size = 2\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ray import tune\n",
        "from transformers.integrations import WandbCallback\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer,padding=True,)\n",
        "\n",
        "\n",
        "\n",
        "for batch_size in [1]:\n",
        "  for weight_picks_1 in [2.]:\n",
        "    for label_smoothing_factor in [.0]:\n",
        "        \n",
        "\n",
        "      training_args = CustomTrainingArguments(\n",
        "        output_dir='./',          # output directory to where save model checkpoint\n",
        "        evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
        "        overwrite_output_dir=True,      \n",
        "        num_train_epochs=1,            # number of training epochs, feel free to tweak\n",
        "        per_device_train_batch_size=batch_size, # the training batch size, put it as high as your GPU memory fits\n",
        "        gradient_accumulation_steps=1,  # accumulating the gradients before updating the weights\n",
        "        per_device_eval_batch_size=2,  # evaluation batch size\n",
        "        logging_steps=50,             # evaluate, log and save model checkpoints every 1000 step\n",
        "        save_steps=200,\n",
        "        gradient_checkpointing = True,\n",
        "        report_to='wandb',\n",
        "        weight_picks_0 = 1.,\n",
        "        weight_picks_1 = weight_picks_1,\n",
        "        label_smoothing_factor = label_smoothing_factor,\n",
        "        run_name=(f'pick1_{weight_picks_1}_smooth{label_smoothing_factor}_bs{batch_size}')\n",
        "\n",
        "\n",
        "\n",
        "      #load_best_model_at_end=False,  # whether to load the best model (in terms of loss) at the end of training\n",
        "      # save_total_limit=3,\n",
        "      #  weight_picks_0 = 3.,\n",
        "      #  weight_picks_1= 6.,\n",
        "      #label_smoothing_factor=.05\n",
        "                  # whether you don't have much space so you let only 3 model weights saved in the disk\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=f1,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator)\n",
        "    \n",
        "    trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    \n",
        "    print(f'pred: {pred} \\n label:{labels}')\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    rounded_labels=np.argmax(labels, axis=1)\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "j8b2Ul9ADwVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "867-FbhxamOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GwGmhAl1YyVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RtuWGpeFans-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1.name = 'f1'"
      ],
      "metadata": {
        "id": "Mj-Gn2ciHEfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1.metric = 'f1'"
      ],
      "metadata": {
        "id": "qHqPd4yOcoN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0rvf1fbVNo"
      },
      "outputs": [],
      "source": [
        "#model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def accuracy(predictions):\n",
        "  class_probs = predictions[0]\n",
        "  true_labels = predictions[1]\n",
        "\n",
        "  f1s = []\n",
        "  for i in range(len(class_probs)):\n",
        "    cp = class_probs[i]\n",
        "    tl = true_labels[i]\n",
        "    pred_labels = [np.argmax(x) for x in class_probs[0]]\n",
        "    f1s.append(f1_score(tl,pred_labels))\n",
        "  return {'accuracy':np.mean(f1s)}\n",
        "\n",
        "# trainer = CustomTrainer(\n",
        "    \n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     data_collator=data_collator,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=test_dataset,\n",
        "#     compute_metrics  = fix_metric,\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# trainer.pop_callback(WandbCallback)\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import f1_score\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "pGv5Z8sP6bB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437d96d9-a206-4c0c-c2fd-a14f1fa2a6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 77.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 96.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 89.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 97.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.2.0 fsspec-2022.1.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "nFdAeVUx6c-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "SwYWtEkV9H-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xfYQ1I5wHT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79051d87-74d6-40e2-cf22-8a4f1348f8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/Rostlab/prot_bert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/26014e822b9642c94410e0829f3080e5aa601146ffdff9eaafda9ca18a3394e7.baf557855a8618d0ddfb6c23bfd135bfc38ccf8c3fb099b8df45eb110ccf05e9\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Rostlab/prot_bert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e0fb1fe71ed3ee73280750839b42681fbc1c276201aedc10d743b356f078520a.f49f0d5b042c555d2d68e50f1e01e6466f521dd1302884e79d128904c6698245\n",
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# from ray import tune\n",
        "# from transformers.integrations import WandbCallback\n",
        "\n",
        "def model_init():\n",
        "    model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)\n",
        "    return model\n",
        "\n",
        "# def hp_space_fn(*args, **kwargs):\n",
        "#     config = {\n",
        "#                 \"learning_rate\": tune.choice([2e-5, 3e-5]),\n",
        "#                 \"num_train_epochs\": tune.choice([.005]),\n",
        "#                  'label_smoothing_factor': tune.choice([0.01,0.03,.07,.10]),\n",
        "#                  'weight_picks_0' : tune.choice([0.01,0.03]),\n",
        "#                  'weight_picks_1' : tune.choice([0.05,.07,.10,.2,.5]),\n",
        "#     }\n",
        "#     wandb_config = {\n",
        "#             \"wandb\": {\n",
        "#                     \"project\": os.environ.get(\n",
        "#                         \"WANDB_PROJECT\",\n",
        "#                         \"wandb_project\"),\n",
        "#                     \"api_key\": os.environ.get(\"API_KEY\"),\n",
        "#                     \"log_config\": True\n",
        "#                     }\n",
        "#     }\n",
        "\n",
        "#     config.update(wandb_config)\n",
        "#     return config\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=f1,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator)\n",
        "\n",
        "# #trainer.pop_callback(WandbCallback)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_LOGGERS"
      ],
      "metadata": {
        "id": "yhBfUBJBY7iE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "72f25e18-b590-4ce5-ef36-15175859242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0f407b2365d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDEFAULT_LOGGERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'DEFAULT_LOGGERS' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZCbsp9XwYr0"
      },
      "outputs": [],
      "source": [
        "# from ray.tune.logger import DEFAULT_LOGGERS\n",
        "# from ray.tune.integration.wandb import WandbLogger\n",
        "# from ray.tune.schedulers import PopulationBasedTraining\n",
        "# import os\n",
        "\n",
        "\n",
        "\n",
        "# #Trial returned a result which did not include the specified metric(s) `f1`\n",
        "# best_run = trainer.hyperparameter_search(\n",
        "#             direction=\"minimize\",\n",
        "#             backend=\"ray\",\n",
        "#             hp_space=hp_space_fn,\n",
        "#             loggers=(DEFAULT_LOGGERS + WandbLogger,),\n",
        "#             #compute_metrics = accuracy,\n",
        "#             n_trials=1#+ (WandbLogger, ),\n",
        "#     )\n",
        "\n",
        "# #logger.info(json.dumps(best_run.hyperparameters, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %env WANDB_PROJECT=test\n",
        "# from ray.tune.logger import DEFAULT_LOGGERS\n",
        "\n",
        "\n",
        "# training_args = CustomTrainingArguments(\n",
        "#     output_dir='./',          # output directory to where save model checkpoint\n",
        "#     evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
        "#     overwrite_output_dir=True,      \n",
        "#     #num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
        "#     per_device_train_batch_size=1, # the training batch size, put it as high as your GPU memory fits\n",
        "#     gradient_accumulation_steps=1,  # accumulating the gradients before updating the weights\n",
        "#     per_device_eval_batch_size=4,  # evaluation batch size\n",
        "#     logging_steps=10,             # evaluate, log and save model checkpoints every 1000 step\n",
        "#     save_steps=50,\n",
        "#     gradient_checkpointing = True,\n",
        "#     #report_to='wandb',\n",
        "# #\n",
        "\n",
        "#      #load_best_model_at_end=False,  # whether to load the best model (in terms of loss) at the end of training\n",
        "#      #save_total_limit=3,\n",
        "#     #  weight_picks_0 = 3.,\n",
        "#     #  weight_picks_1= 6.,\n",
        "#      #label_smoothing_factor=.05\n",
        "#                 # whether you don't have much space so you let only 3 model weights saved in the disk\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from ray import tune\n",
        "# from transformers.integrations import WandbCallback\n",
        "\n",
        "# def model_init():\n",
        "#     model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)\n",
        "#     return model\n",
        "\n",
        "# def hp_space_fn(*args, **kwargs):\n",
        "#     config = {\n",
        "#                 \"learning_rate\": tune.choice([2e-5, 3e-5]),\n",
        "#                 \"num_train_epochs\": tune.choice([.005]),\n",
        "#                 'label_smoothing_factor': tune.choice([0.01,0.03,.07,.10]),\n",
        "#                 'weight_picks_0' : tune.choice([0.01,0.03]),\n",
        "#                 'weight_picks_1' : tune.choice([0.05,.07,.10,.2,.5]),\n",
        "#     }\n",
        "#     wandb_config = {\n",
        "#             \"wandb\": {\n",
        "#                     \"project\": os.environ.get(\n",
        "#                         \"WANDB_PROJECT\",\n",
        "#                         \"wandb_project\"),\n",
        "#                     \"api_key\": os.environ.get(\"API_KEY\"),\n",
        "#                     \"log_config\": False\n",
        "#                     }\n",
        "#     }\n",
        "\n",
        "#     config.update(wandb_config)\n",
        "#     return config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def hp_space_fn(*args, **kwargs):\n",
        "#     config = {\n",
        "#                 \"learning_rate\": tune.choice([2e-5, 3e-5]),\n",
        "#                 \"num_train_epochs\": tune.choice([.005]),\n",
        "#                 'label_smoothing_factor': tune.choice([0.01,0.03,.07,.10]),\n",
        "#                 'weight_picks_0' : tune.choice([0.01,0.03]),\n",
        "#                 'weight_picks_1' : tune.choice([0.05,.07,.10,.2,.5]),\n",
        "#     }\n",
        "#     wandb_config = {\n",
        "#             \"wandb\": {\n",
        "#                     \"project\": os.environ.get(\n",
        "#                         \"WANDB_PROJECT\",\n",
        "#                         \"wandb_project\"),\n",
        "#                     \"api_key\": os.environ.get(\"API_KEY\"),\n",
        "#                     \"log_config\": True\n",
        "#                     }\n",
        "#     }\n",
        "#     config.update(wandb_config)\n",
        "#     return config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# trainer = CustomTrainer(\n",
        "#         model_init=model_init,\n",
        "#         args=training_args,\n",
        "#         train_dataset=train_dataset,\n",
        "#         eval_dataset=test_dataset,\n",
        "#         compute_metrics=f1,\n",
        "#         tokenizer=tokenizer,\n",
        "#         data_collator=data_collator)\n",
        "\n",
        "# trainer.train()\n",
        "# #trainer.pop_callback(WandbCallback)\n",
        "\n",
        "# from ray.tune.logger import DEFAULT_LOGGERS\n",
        "# from ray.tune.integration.wandb import WandbLogger\n",
        "# from ray.tune.schedulers import PopulationBasedTraining\n",
        "# import os\n",
        "\n",
        "\n",
        "# #Trial returned a result which did not include the specified metric(s) `f1`\n",
        "# best_run = trainer.hyperparameter_search(\n",
        "#             direction=\"minimize\",\n",
        "#             backend=\"ray\",\n",
        "#             hp_space=hp_space_fn,\n",
        "#             loggers=DEFAULT_LOGGERS + (WandbLogger,),\n",
        "#             #compute_metrics = accuracy,\n",
        "#             n_trials=2#+ (WandbLogger, ),\n",
        "#     )\n",
        "\n",
        "# #logger.info(json.dumps(best_run.hyperparameters, indent=4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_N79IwWucL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6fSCMds1qqTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=sing\n",
        "from ray.tune.logger import DEFAULT_LOGGERS\n",
        "\n",
        "\n",
        "training_args = CustomTrainingArguments(\n",
        "    output_dir='./',          # output directory to where save model checkpoint\n",
        "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
        "    overwrite_output_dir=True,      \n",
        "    #num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
        "    per_device_train_batch_size=1, # the training batch size, put it as high as your GPU memory fits\n",
        "    gradient_accumulation_steps=1,  # accumulating the gradients before updating the weights\n",
        "    per_device_eval_batch_size=4,  # evaluation batch size\n",
        "    logging_steps=10,             # evaluate, log and save model checkpoints every 1000 step\n",
        "    save_steps=50,\n",
        "    gradient_checkpointing = True,\n",
        "    weight_picks_0 = 1.,\n",
        "    weight_picks_1=7.,\n",
        "    #report_to='wandb',\n",
        "#\n",
        "\n",
        "     #load_best_model_at_end=False,  # whether to load the best model (in terms of loss) at the end of training\n",
        "     #save_total_limit=3,\n",
        "    #  weight_picks_0 = 3.,\n",
        "    #  weight_picks_1= 6.,\n",
        "     #label_smoothing_factor=.05\n",
        "                # whether you don't have much space so you let only 3 model weights saved in the disk\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from ray import tune\n",
        "from transformers.integrations import WandbCallback\n",
        "\n",
        "def model_init():\n",
        "    model = BertForTokenClassification.from_pretrained(\"Rostlab/prot_bert\",)\n",
        "    return model\n",
        "\n",
        "def hp_space_fn(*args, **kwargs):\n",
        "    config = {\n",
        "                \"learning_rate\": tune.choice([2e-5, 3e-5]),\n",
        "                \"num_train_epochs\": tune.choice([.005]),\n",
        "                'label_smoothing_factor': tune.choice([0.01,0.03,.07,.10]),\n",
        "                'weight_picks_0' : tune.choice([0.01,0.03]),\n",
        "                'weight_picks_1' : tune.choice([0.05,.07,.10,.2,.5]),\n",
        "    }\n",
        "    wandb_config = {\n",
        "            \"wandb\": {\n",
        "                    \"project\": os.environ.get(\n",
        "                        \"WANDB_PROJECT\",\n",
        "                        \"wandb_project\"),\n",
        "                    \"api_key\": os.environ.get(\"API_KEY\"),\n",
        "                    \"log_config\": True\n",
        "                    }\n",
        "    }\n",
        "\n",
        "    config.update(wandb_config)\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def hp_space_fn(*args, **kwargs):\n",
        "    config = {\n",
        "                \"learning_rate\": tune.choice([2e-5, 3e-5]),\n",
        "                \"num_train_epochs\": tune.choice([.005]),\n",
        "                'label_smoothing_factor': tune.choice([0.01,0.03,.07,.10]),\n",
        "                'weight_picks_0' : tune.choice([0.01,0.03]),\n",
        "                'weight_picks_1' : tune.choice([0.05,.07,.10,.2,.5]),\n",
        "    }\n",
        "    wandb_config = {\n",
        "            \"wandb\": {\n",
        "                    \"project\": os.environ.get(\n",
        "                        \"WANDB_PROJECT\",\n",
        "                        \"wandb_project\"),\n",
        "                    \"api_key\": os.environ.get(\"API_KEY\"),\n",
        "                    \"log_config\": True\n",
        "                    }\n",
        "    }\n",
        "    config.update(wandb_config)\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=f1,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "pGK08COkqqWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d-yWdOGnqqY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nor_-ULfqqbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "odcwsWlPqqgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hp_space_fn(*args, **kwargs):\n",
        "    config = {\n",
        "                \"learning_rate\": tune.choice([2e-5, 3e-5]),\n",
        "                \"num_train_epochs\": tune.choice([.005]),\n",
        "                 'label_smoothing_factor': tune.choice([0.01,0.03,.07,.10]),\n",
        "                 'weight_picks_0' : tune.choice([0.01,0.03]),\n",
        "                 'weight_picks_1' : tune.choice([0.05,.07,.10,.2,.5]),\n",
        "    }\n",
        "    wandb_config = {\n",
        "            \"wandb\": {\n",
        "                    \"project\": os.environ.get(\n",
        "                        \"WANDB_PROJECT\",\n",
        "                        \"wandb_project\"),\n",
        "                    \"api_key\": os.environ.get(\"API_KEY\"),\n",
        "                    \"log_config\": True\n",
        "                    }\n",
        "    }\n",
        "\n",
        "    config.update(wandb_config)\n",
        "    return config\n",
        "\n",
        "x = hp_space_fn()"
      ],
      "metadata": {
        "id": "WIdC1g8_ckEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "A80SFIiVcmk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf1mxAGQ0fdw"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mylog = WandbLogger\n",
        "mylog.update_config(mylog, config=hp_space_fn())"
      ],
      "metadata": {
        "id": "0YDWW575g-hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mylog"
      ],
      "metadata": {
        "id": "XyRoWGYcaV-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp_space_fn()"
      ],
      "metadata": {
        "id": "qFBfgaT3Zqt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-8YERO9MaB_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "finetune_protbert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}